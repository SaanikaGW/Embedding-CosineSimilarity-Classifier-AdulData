{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82391209",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Sequence, Tuple, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def run_bias_curve(\n",
    "    df: pd.DataFrame,\n",
    "    percent_points: int = 20,\n",
    "    classifiers: Sequence[str] = (\"SVC\", \"LR\", \"LinearRegression\",\n",
    "                                  \"RandomForest\", \"GradientBoost\", \"MLP\"),\n",
    "    model_kwargs: Dict[str, Dict[str, Any]] | None = None,\n",
    "    random_state: int | None = 0,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute bias vs. sampling-percent curves for a set of models.\n",
    "\n",
    "    DataFrame requirements:\n",
    "      df columns = ['dataSet','gender','sentiment','embedding']\n",
    "      - 'dataSet' in {'train','test'}\n",
    "      - 'gender' in {'boys','girls'}\n",
    "      - 'sentiment' in {'positive','negative'}\n",
    "      - 'embedding' is an array-like vector (same length per row)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bias_df : pd.DataFrame columns ['percent','bias','classifier']\n",
    "    fig     : plotly Figure\n",
    "    \"\"\"\n",
    "    if model_kwargs is None:\n",
    "        model_kwargs = {}\n",
    "\n",
    "    # --- helpers --------------------------------------------------------------\n",
    "    def _to_X_y(sliced_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X = np.stack(sliced_df[\"embedding\"].values)\n",
    "        # map -> numpy to avoid FutureWarning\n",
    "        y = sliced_df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0}).to_numpy(np.int8)\n",
    "        return X, y\n",
    "\n",
    "    def _logistic(x: np.ndarray) -> np.ndarray:\n",
    "        z = np.clip(x, -50, 50)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def _minmax01(x: np.ndarray) -> np.ndarray:\n",
    "        mn, mx = np.min(x), np.max(x)\n",
    "        if mx == mn:\n",
    "            return np.full_like(x, 0.5, dtype=float)\n",
    "        return (x - mn) / (mx - mn)\n",
    "\n",
    "    def _get_model(name: str):\n",
    "        key = name.lower()\n",
    "        if key in {\"lr\", \"logreg\", \"logistic\", \"logisticregression\"}:\n",
    "            kw = {**model_kwargs.get(\"LR\", {}), **model_kwargs.get(\"LogisticRegression\", {})}\n",
    "            return LogisticRegression(max_iter=200, random_state=random_state, **kw)\n",
    "        if key == \"svc\":\n",
    "            # default probability=False for speed; user can override via model_kwargs[\"SVC\"][\"probability\"]=True\n",
    "            default = {\"probability\": False}\n",
    "            kw = {**default, **model_kwargs.get(\"SVC\", {})}\n",
    "            return SVC(random_state=random_state, **kw)\n",
    "        if key in {\"rf\", \"randomforest\"}:\n",
    "            default = {\"n_jobs\": -1}\n",
    "            kw = {**default, **model_kwargs.get(\"RandomForest\", {})}\n",
    "            return RandomForestClassifier(random_state=random_state, **kw)\n",
    "        if key in {\"gb\", \"gradientboost\", \"gradientboosting\"}:\n",
    "            return GradientBoostingClassifier(random_state=random_state, **model_kwargs.get(\"GradientBoost\", {}))\n",
    "        if key in {\"mlp\", \"mlpclassifier\"}:\n",
    "            default = {\"max_iter\": 200}\n",
    "            kw = {**default, **model_kwargs.get(\"MLP\", {})}\n",
    "            return MLPClassifier(random_state=random_state, **kw)\n",
    "        if key in {\"linear\", \"linearregression\"}:\n",
    "            return LinearRegression(**model_kwargs.get(\"LinearRegression\", {}))\n",
    "        raise ValueError(f\"Unknown classifier '{name}'.\")\n",
    "\n",
    "    def _scores_from_model(model, X_train, y_train, X_test) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Try predict_proba -> decision_function (logistic) -> predict (minmax+clip).\n",
    "        Returns scores in [0,1] aligned with 'positive' class.\n",
    "        \"\"\"\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            probs = model.predict_proba(X_test)\n",
    "            if probs.ndim == 2 and probs.shape[1] >= 2:\n",
    "                return probs[:, 1]\n",
    "\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            df_out = model.decision_function(X_test)\n",
    "            if np.ndim(df_out) == 1:\n",
    "                return _logistic(df_out)\n",
    "            return _logistic(df_out[:, -1])\n",
    "\n",
    "        preds = model.predict(X_test).astype(float)\n",
    "        if np.any((preds < 0) | (preds > 1)):\n",
    "            preds = _minmax01(preds)\n",
    "        return np.clip(preds, 0.0, 1.0)\n",
    "\n",
    "    # --- data splits ----------------------------------------------------------\n",
    "    train_data = df[df[\"dataSet\"] == \"train\"].copy()\n",
    "    test_data_master = df[df[\"dataSet\"] == \"test\"].copy()\n",
    "\n",
    "    # confirm all four cells exist in TRAIN\n",
    "    counts = (\n",
    "        train_data\n",
    "        .groupby([\"gender\", \"sentiment\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    for g in (\"boys\", \"girls\"):\n",
    "        if g not in counts.index:\n",
    "            counts.loc[g] = 0\n",
    "    for s in (\"positive\", \"negative\"):\n",
    "        if s not in counts.columns:\n",
    "            counts[s] = 0\n",
    "    counts = (\n",
    "        counts.sort_index()\n",
    "              .reindex(index=[\"boys\", \"girls\"], columns=[\"positive\", \"negative\"])\n",
    "              .fillna(0)\n",
    "    )\n",
    "\n",
    "    min_per_cell = int(counts.min().min()) if counts.size else 0\n",
    "    if verbose:\n",
    "        print(\"Per-cell TRAIN counts:\\n\", counts)\n",
    "        print(\"min_per_cell:\", min_per_cell)\n",
    "    if min_per_cell == 0:\n",
    "        raise ValueError(\"Training data must include all four (gender, sentiment) cells to sample.\")\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    def sample_training_data(train_df: pd.DataFrame, pct: float) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        boys:  positive = p, negative = 1-p\n",
    "        girls: positive = 1-p, negative = p\n",
    "        \"\"\"\n",
    "        n_pos_boys = int(round(pct * min_per_cell))\n",
    "        n_neg_boys = min_per_cell - n_pos_boys\n",
    "        n_pos_girls = n_neg_boys\n",
    "        n_neg_girls = n_pos_boys\n",
    "\n",
    "        def pick(gender, sentiment, n):\n",
    "            pool = train_df[(train_df.gender == gender) & (train_df.sentiment == sentiment)]\n",
    "            return pool.sample(n=n, replace=False, random_state=int(rng.integers(0, 2**31 - 1)))\n",
    "\n",
    "        return pd.concat(\n",
    "            [\n",
    "                pick(\"boys\", \"positive\", n_pos_boys),\n",
    "                pick(\"boys\", \"negative\", n_neg_boys),\n",
    "                pick(\"girls\", \"positive\", n_pos_girls),\n",
    "                pick(\"girls\", \"negative\", n_neg_girls),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    # Precompute fixed TEST arrays & boolean masks to avoid per-iter DataFrame work\n",
    "    test_X, test_y = _to_X_y(test_data_master)  # y unused for scoring but kept for completeness\n",
    "    t_gender = test_data_master[\"gender\"].to_numpy()\n",
    "    t_sent   = test_data_master[\"sentiment\"].to_numpy()\n",
    "\n",
    "    mask_neg_boys  = (t_sent == \"negative\") & (t_gender == \"boys\")\n",
    "    mask_neg_girls = (t_sent == \"negative\") & (t_gender == \"girls\")\n",
    "    mask_pos_boys  = (t_sent == \"positive\") & (t_gender == \"boys\")\n",
    "    mask_pos_girls = (t_sent == \"positive\") & (t_gender == \"girls\")\n",
    "\n",
    "    def _mean_or_zero(arr: np.ndarray, mask: np.ndarray) -> float:\n",
    "        if not np.any(mask):\n",
    "            return 0.0\n",
    "        return float(arr[mask].mean())\n",
    "\n",
    "    # --- main loop ------------------------------------------------------------\n",
    "    bias_rows = []\n",
    "    percents = np.linspace(0.0, 1.0, percent_points)\n",
    "\n",
    "    for pct in percents:\n",
    "        train_sample = sample_training_data(train_data, float(pct))\n",
    "        train_X, train_y = _to_X_y(train_sample)\n",
    "\n",
    "        for name in classifiers:\n",
    "            model = _get_model(name)\n",
    "            scores = _scores_from_model(model, train_X, train_y, test_X)\n",
    "\n",
    "            neg_boys  = _mean_or_zero(scores, mask_neg_boys)\n",
    "            neg_girls = _mean_or_zero(scores, mask_neg_girls)\n",
    "            pos_boys  = _mean_or_zero(scores, mask_pos_boys)\n",
    "            pos_girls = _mean_or_zero(scores, mask_pos_girls)\n",
    "\n",
    "            num_bias = (neg_boys + pos_boys) - (neg_girls + pos_girls)\n",
    "            bias_rows.append({\"percent\": float(pct), \"bias\": num_bias, \"classifier\": name})\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"pct={pct:.2f} done\")\n",
    "\n",
    "    bias_df = pd.DataFrame(bias_rows)\n",
    "    fig = px.line(\n",
    "        bias_df, x=\"percent\", y=\"bias\", color=\"classifier\",\n",
    "        title=\"Bias vs. Sampling Percent across Models (boys: p pos; girls: 1-p)\"\n",
    "    )\n",
    "    return bias_df, fig\n",
    "\n",
    "# --- usage -------------------------------------------------------------------\n",
    "gender_bias_df = pd.read_pickle(\"data_set_with_embeddings.pickle\").dropna()\n",
    "bias_df, fig = run_bias_curve(\n",
    "    gender_bias_df,\n",
    "    percent_points=21,\n",
    "    classifiers=(\"SVC\",\"LR\",\"LinearRegression\",\"RandomForest\",\"GradientBoost\",\"MLP\"),\n",
    "    model_kwargs={\n",
    "        \"SVC\": {\"C\": 1.0, \"kernel\": \"rbf\"},                   # add \"probability\": True only if you need it\n",
    "        \"RandomForest\": {\"n_estimators\": 300, \"max_depth\": None, \"n_jobs\": -1},\n",
    "        \"MLP\": {\"hidden_layer_sizes\": (128, 64), \"max_iter\": 200},\n",
    "    },\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(bias_df.head())\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
